groups:
  - name: orders-service
    rules:
      - alert: OrdersServiceDown
        expr: up{job="orders"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Orders service is down"
          description: "Prometheus cannot scrape orders service on {{ $labels.instance }} for 1+ minutes"

      - alert: OrdersHigh5xxRate
        expr: |
          (
            sum(rate(http_requests_total{job="orders",status_class="5xx"}[5m])) /
            sum(rate(http_requests_total{job="orders"}[5m]))
          ) * 100 > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High 5xx error rate on orders service"
          description: "Orders service 5xx error rate above 10% for 5 minutes"

      - alert: OrdersHighLatencyP95
        expr: |
          histogram_quantile(0.95, 
            sum by (le) (
              rate(http_request_duration_seconds_bucket{job="orders"}[5m])
            )
          ) > 1.0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency on orders service"
          description: "Orders service P95 latency above 1s for 5 minutes"

  - name: monitoring-health
    rules:
      - alert: PrometheusTargetDown
        expr: up{job="prometheus"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus self-monitoring failed"
          description: "Prometheus cannot scrape its own metrics for 2+ minutes"